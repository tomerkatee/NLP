{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM85j1-ZX8RZ"
      },
      "source": [
        "To begin copy this notebook to your own drive:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "### Submission Instructions:\n",
        "1. **Restart the kernel** (in the menubar, select Runtime$\\rightarrow$Restart runtime)\n",
        "2. **Download the notebook** (in the menubar, select File$\\rightarrow$Download .ipynb)\n",
        "3. **Upload the downloaded notebook (.ipynb file) to your repository**.\n",
        "\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE`, and that no tests fail.  \n",
        "\n",
        "Note: To use a GPU, do the following: Runtime$\\rightarrow$Change runtime type$\\rightarrow$ GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOtck6n09468",
        "outputId": "7046c669-8692-4108-9368-11e2c3409c19"
      },
      "source": [
        "import gdown\n",
        "import nltk\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1PFOG06NEsTL6VieKQjMk1oNzyzcUtiWn', 'glove.npy', quiet=False)\n",
        "gdown.download('https://drive.google.com/uc?export=download&id=1-3SxpirQjmX-RCRyRjKdP2L7G_tNgp00', 'vocab.json', quiet=False)\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1PFOG06NEsTL6VieKQjMk1oNzyzcUtiWn\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1PFOG06NEsTL6VieKQjMk1oNzyzcUtiWn&confirm=t&uuid=f592a6f0-1a6e-4286-8f16-44525ff16214\n",
            "To: /content/glove.npy\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480M/480M [00:02<00:00, 163MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1-3SxpirQjmX-RCRyRjKdP2L7G_tNgp00\n",
            "To: /content/vocab.json\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.69M/7.69M [00:00<00:00, 195MB/s]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbkSh2AW-tpN",
        "outputId": "b8fb672b-1dca-48d5-94b1-9ef83df7cfa7"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdk6SawN-Ahb",
        "outputId": "b29a36ff-686f-4310-faa2-53fd322a0a1f"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnx3G7V8-PiR"
      },
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "with open(\"vocab.json\") as f:\n",
        "  vocab = json.load(f)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  sentences = [x.lower() for x in example['text']]\n",
        "  tokenized_sentences = [word_tokenize(x) for x in sentences]\n",
        "  tokenized_idx = [[vocab[word] if word in vocab else vocab[\"unk\"] for word in x] for x in tokenized_sentences]\n",
        "  max_size = max([len(x) for x in tokenized_idx])\n",
        "  final_tokenized_idx = tokenized_idx\n",
        "\n",
        "  return {\"labels\":example['label'],'input_ids':final_tokenized_idx}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB21r_h3A7yB"
      },
      "source": [
        "small_train_dataset = raw_datasets['train'].shuffle(seed=42).map(tokenize_function,batched=True)\n",
        "small_eval_dataset = raw_datasets['test'].shuffle(seed=42).map(tokenize_function,batched=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI0aXI_ip2v1"
      },
      "source": [
        "def pad_sequence_to_length(\n",
        "    sequence,\n",
        "    desired_length: int,\n",
        "    default_value = lambda: 0,\n",
        "    padding_on_right: bool = True,\n",
        "):\n",
        "    sequence = list(sequence)\n",
        "    # Truncates the sequence to the desired length.\n",
        "    if padding_on_right:\n",
        "        padded_sequence = sequence[:desired_length]\n",
        "    else:\n",
        "        padded_sequence = sequence[-desired_length:]\n",
        "    # Continues to pad with default_value() until we reach the desired length.\n",
        "    pad_length = desired_length - len(padded_sequence)\n",
        "    # This just creates the default value once, so if it's a list, and if it gets mutated\n",
        "    # later, it could cause subtle bugs. But the risk there is low, and this is much faster.\n",
        "    values_to_pad = [default_value()] * pad_length\n",
        "    if padding_on_right:\n",
        "        padded_sequence = padded_sequence + values_to_pad\n",
        "    else:\n",
        "        padded_sequence = values_to_pad + padded_sequence\n",
        "    return padded_sequence"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xGIcwomAG3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b70e72f-8909-44c2-a5fe-859804559999"
      },
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-c6aba47c7ef1>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnOoHih5WzQf"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorWithPadding:\n",
        "\n",
        "  def __call__(self, features):\n",
        "    features_dict={}\n",
        "    if \"labels\" in features[0]:\n",
        "\n",
        "      features_dict[\"labels\"] = torch.tensor([x.pop(\"labels\") for x in features]).long()\n",
        "\n",
        "    input_ids = [x.pop(\"input_ids\") for x in features]\n",
        "    max_len = max(len(x) for x in input_ids)\n",
        "    masks = [[1]*len(x) for x in input_ids]\n",
        "\n",
        "    features_dict[\"input_ids\"] = torch.tensor([pad_sequence_to_length(x,max_len) for x in input_ids]).long()\n",
        "    features_dict[\"attention_masks\"] = torch.tensor([pad_sequence_to_length(x,max_len) for x in masks]).long()\n",
        "\n",
        "    return features_dict\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC1VDdd1DfSQ"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "#Use nn.Sequential and nn.Linear for the network, and nn.CrossEntropyLoss for the loss.\n",
        "#Make sure that the final layer has output dimension of size 2.\n",
        "class DAN(nn.Module):\n",
        "    def __init__(self, p=0, num_hidden_layers=1, hidden_size=256, activation_func=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.num_labels = 2\n",
        "        self.p = p\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.activation_func = activation_func\n",
        "\n",
        "        # Define the embedding layer (assuming GloVe embeddings are used)\n",
        "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(np.load(\"glove.npy\")))\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        layers = []\n",
        "        if self.num_hidden_layers == 0:\n",
        "            layers = [nn.Linear(self.embeddings.embedding_dim, self.num_labels)]\n",
        "        else:\n",
        "            for i in range(num_hidden_layers):\n",
        "                if i == 0:\n",
        "                    layers.append(nn.Linear(self.embeddings.embedding_dim, self.hidden_size))\n",
        "                else:\n",
        "                    layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
        "                    layers.append(self.activation_func)\n",
        "            layers.append(nn.Linear(self.hidden_size, self.num_labels))\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        # END YOUR END\n",
        "\n",
        "    def forward(self, input_ids, attention_masks, labels=None, **kwargs):\n",
        "        # YOUR CODE HERE\n",
        "        embeddings = self.embeddings(input_ids)\n",
        "        if self.training:\n",
        "            mask = torch.bernoulli(torch.ones_like(attention_masks, dtype=torch.float) * (1 - self.p))\n",
        "            attention_masks = attention_masks.float() * mask\n",
        "            embeddings = embeddings * mask.unsqueeze(-1)\n",
        "        avg = torch.sum(embeddings, dim=1) / torch.sum(attention_masks, dim=1).unsqueeze(-1)\n",
        "        # END YOUR END\n",
        "\n",
        "        res = self.classifier(avg)\n",
        "        loss = self.loss(res, labels)\n",
        "        return {\"loss\": loss, \"logits\": res}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTle329KAw19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae1a5b1-ca95-444c-cf4c-b515b03aad14"
      },
      "source": [
        "#Hint: You may want to look at https://huggingface.co/transformers/main_classes/callback.html\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers.integrations import TensorBoardCallback\n",
        "\n",
        "\n",
        "co = DataCollatorWithPadding()\n",
        "training_args = TrainingArguments(\"DAN\",\n",
        "                                  # YOUR CODE HERE\n",
        "                                  num_train_epochs=20, #must be at least 10.\n",
        "                                  per_device_train_batch_size=128,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  learning_rate=0.01,\n",
        "                                  logging_strategy=\"epoch\",\n",
        "                                  # END YOUR END\n",
        "\n",
        "                                  save_total_limit=2,\n",
        "                                  log_level=\"error\",\n",
        "                                  evaluation_strategy=\"epoch\")\n",
        "model = DAN()\n",
        "\n",
        "\n",
        "\n",
        "trainer1 = Trainer(\n",
        "    model=model,\n",
        "    data_collator=co,\n",
        "    args=training_args,\n",
        "    callbacks = [\n",
        "                 # YOUR CODE HERE\n",
        "                  TensorBoardCallback\n",
        "                 # END YOUR END\n",
        "    ],\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HvPThWvCq1F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "baa5a8a6-b7c1-4737-ef64-c2f23e13a59c"
      },
      "source": [
        "trainer1.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='589' max='3920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 589/3920 01:29 < 08:26, 6.58 it/s, Epoch 3/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.497000</td>\n",
              "      <td>0.514825</td>\n",
              "      <td>0.751960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.413200</td>\n",
              "      <td>0.398459</td>\n",
              "      <td>0.822280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1388' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1388/3125 00:06 < 00:08, 204.78 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Extract evaluation accuracies from the log history\n",
        "eval_accuracy = [d['eval_accuracy'] for d in trainer1.state.log_history if 'eval_accuracy' in d]\n",
        "\n",
        "# Determine the maximum accuracy\n",
        "max_accuracy = np.max(eval_accuracy)\n",
        "\n",
        "# Plot the evaluation accuracies\n",
        "plt.plot(np.arange(1, len(eval_accuracy) + 1), eval_accuracy, label='Eval Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Eval Accuracy')\n",
        "\n",
        "# Add a horizontal line at the maximum accuracy\n",
        "plt.axhline(y=max_accuracy, color='r', linestyle='dashed', label=f'Max Accuracy: {max_accuracy:.2f}')\n",
        "\n",
        "# Include the maximum accuracy in the y-axis ticks\n",
        "plt.yticks(list(plt.yticks()[0]) + [max_accuracy])\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ikyoOxS-g6dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "\n",
        "#Redifining DAN():\n",
        "class DynamicDAN(nn.Module):\n",
        "  def __init__(self, p=0.0, hidden_layers=1, hidden_size=[256], activation_func=nn.ReLU()):\n",
        "          super().__init__()\n",
        "          self.num_labels = 2\n",
        "          self.p = p\n",
        "          self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(np.load(\"glove.npy\")))\n",
        "          # YOUR CODE HERE\n",
        "          self.embedding_size = torch.FloatTensor(np.load(\"glove.npy\")).size(1)\n",
        "\n",
        "          classifier_args = []\n",
        "          if hidden_layers == 0:\n",
        "              classifier_args = [nn.Linear(self.embeddings.embedding_dim, self.num_labels)]\n",
        "          else:\n",
        "              for i in range(hidden_layers):\n",
        "                  if i == 0:\n",
        "                      classifier_args.append(nn.Linear(self.embeddings.embedding_dim, hidden_size[i]))\n",
        "                  else:\n",
        "                      classifier_args.append(nn.Linear(in_features=hidden_size[i-1], out_features=hidden_size[i]))\n",
        "                  classifier_args.append(activation_func)\n",
        "\n",
        "              classifier_args.append(nn.Linear(hidden_size[len(hidden_size)-1], self.num_labels))\n",
        "\n",
        "          self.classifier = nn.Sequential(*classifier_args)\n",
        "          self.loss = nn.CrossEntropyLoss()\n",
        "          # END YOUR END\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,input_ids,attention_masks,labels=None,**kwargs):\n",
        "      # YOUR CODE HERE\n",
        "      # Embedding Lookup\n",
        "      embeddings = self.embeddings(input_ids)\n",
        "      if self.training:\n",
        "            dropout_mask = torch.bernoulli(torch.ones_like(attention_masks) * (1 - self.p)).unsqueeze(-1).float()\n",
        "            embeddings = embeddings * dropout_mask\n",
        "\n",
        "      masked_embeddings = embeddings * attention_masks.unsqueeze(-1).float()\n",
        "\n",
        "      sum_embeddings = masked_embeddings.sum(dim=1)\n",
        "      sum_masks = attention_masks.sum(dim=1).unsqueeze(-1)\n",
        "      avg = sum_embeddings / sum_masks\n",
        "      # END YOUR END\n",
        "      res = self.classifier(avg)\n",
        "      loss = self.loss(res,labels)\n",
        "      return {\"loss\":loss,\"logits\":res}\n"
      ],
      "metadata": {
        "id": "UwhS3V573KzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers.integrations import TensorBoardCallback\n",
        "\n",
        "co = DataCollatorWithPadding()\n",
        "training_args = TrainingArguments(\"DynamicDAN\",\n",
        "                                  # YOUR CODE HERE\n",
        "                                  num_train_epochs= 20, #must be at least 10.\n",
        "                                  per_device_train_batch_size=128,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  learning_rate= 0.01,\n",
        "                                  logging_strategy=\"epoch\",\n",
        "                                  # END YOUR END\n",
        "\n",
        "                                  save_total_limit=2,\n",
        "                                  log_level=\"error\",\n",
        "                                  evaluation_strategy=\"epoch\")\n",
        "\n",
        "\n",
        "\n",
        "def run_with_dropout(p):\n",
        "  model = DynamicDAN(p=p)\n",
        "\n",
        "  trainer = (Trainer(\n",
        "      model=model,\n",
        "      data_collator=co,\n",
        "      args=training_args,\n",
        "      callbacks = [\n",
        "                  # YOUR CODE HERE\n",
        "                  TensorBoardCallback()\n",
        "                  # END YOUR END\n",
        "      ],\n",
        "      train_dataset=small_train_dataset,\n",
        "      eval_dataset=small_eval_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        "  ))\n",
        "  trainer.train()\n",
        "  results = [d['eval_accuracy'] for d in trainer.state.log_history if 'eval_accuracy' in d ]\n",
        "  return results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3DtWYob2q8O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "results.append(run_with_dropout(0.1))\n",
        "results.append(run_with_dropout(0.2))\n",
        "results.append(run_with_dropout(0.3))\n",
        "results.append(run_with_dropout(0.4))\n",
        "results.append(run_with_dropout(0.5))"
      ],
      "metadata": {
        "id": "ZR5RePVR3WT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "ps = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "for i, result in enumerate(results):\n",
        "    plt.plot(np.arange(1, 21), result, label=f\"p={ps[i]:.1f}\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Dropout Prob')\n",
        "plt.ylabel('Mean Accuracy')\n",
        "plt.plot(ps, np.mean(results, axis=1))\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Dropout Prob')\n",
        "plt.ylabel('Last Epoch Accuracy')\n",
        "plt.plot(ps, np.array(results)[:, -1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a1bZtIUqbVMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\"DAN\",\n",
        "                                  # YOUR CODE HERE\n",
        "                                  num_train_epochs=20 , #must be at least 10.\n",
        "                                  per_device_train_batch_size=128,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  learning_rate=0.01,\n",
        "                                  logging_strategy=\"epoch\",\n",
        "                                  # END YOUR END\n",
        "\n",
        "                                  save_total_limit=2,\n",
        "                                  log_level=\"error\",\n",
        "                                  evaluation_strategy=\"epoch\")\n",
        "resultsc = []\n",
        "trainersc = []\n",
        "for num_layers in range(0, 4):\n",
        "  model = DAN(num_hidden_layers=num_layers)\n",
        "  trainersc.append(Trainer(\n",
        "      model=model,\n",
        "      data_collator=co,\n",
        "      args=training_args,\n",
        "      callbacks = [\n",
        "                  # YOUR CODE HERE\n",
        "                    TensorBoardCallback\n",
        "                  # END YOUR END\n",
        "      ],\n",
        "      train_dataset=small_train_dataset,\n",
        "      eval_dataset=small_eval_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        "  ))\n",
        "  trainersc[-1].train()\n",
        "  resultsc.append([d['eval_accuracy'] for d in trainersc[-1].state.log_history if 'eval_accuracy' in d ])\n"
      ],
      "metadata": {
        "id": "klZ9l2I1xKme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Eval Accuracy')\n",
        "for i, result in enumerate(resultsc):\n",
        "  plt.plot(np.arange(1, num_epochs + 1), result, label=f\"{i} hidden layers\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Num Hidden Layers')\n",
        "plt.ylabel('Mean Eval Accuracy')\n",
        "plt.plot(np.arange(4), np.mean(resultsc, axis=1))\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel('Num Hidden Layers')\n",
        "plt.ylabel('Last Epoch Eval Accuracy')\n",
        "plt.plot(np.arange(4), np.array(resultsc)[:, -1])\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GASwYh2R6VnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers.integrations import TensorBoardCallback\n",
        "\n",
        "co = DataCollatorWithPadding()\n",
        "training_args = TrainingArguments(\"DAN\",\n",
        "                                  # YOUR CODE HERE\n",
        "                                  num_train_epochs= 20, #must be at least 10.\n",
        "                                  per_device_train_batch_size=128,\n",
        "                                  per_device_eval_batch_size=8,\n",
        "                                  learning_rate= 0.01,\n",
        "                                  logging_strategy=\"epoch\",\n",
        "                                  # END YOUR END\n",
        "\n",
        "                                  save_total_limit=2,\n",
        "                                  log_level=\"error\",\n",
        "                                  evaluation_strategy=\"epoch\")\n",
        "\n",
        "\n",
        "def run_with_activation_function(activation_function):\n",
        "  model = DynamicDAN(activation_func = activation_function)\n",
        "\n",
        "  trainer_func=(Trainer(\n",
        "      model=model,\n",
        "      data_collator=co,\n",
        "      args=training_args,\n",
        "      callbacks = [\n",
        "                  # YOUR CODE HERE\n",
        "                  TensorBoardCallback()\n",
        "                  # END YOUR END\n",
        "      ],\n",
        "      train_dataset=small_train_dataset,\n",
        "      eval_dataset=small_eval_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        "  ))\n",
        "  trainer_func.train()\n",
        "  results_func = [d['eval_accuracy'] for d in trainer_func.state.log_history if 'eval_accuracy' in d ]\n",
        "  return results_func"
      ],
      "metadata": {
        "id": "jbo6DEnc6vqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_relu = run_with_activation_function(nn.ReLU())\n",
        "results_sigmoid = run_with_activation_function(nn.Sigmoid())\n",
        "results_mish = run_with_activation_function(nn.Mish())"
      ],
      "metadata": {
        "id": "184nEk4Q60y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "resultsd = [results_relu, results_sigmoid, results_mish]\n",
        "activations = ['ReLU', 'Sigmoid', 'Mish']\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Eval Accuracy')\n",
        "for i, result in enumerate(resultsd):\n",
        "  plt.plot(np.arange(1, 21), result, label=f\"{activations[i]}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7r4dKQ7W68cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainera.predict(small_eval_dataset)\n",
        "predicted_labels = np.argmax(predictions[0], axis=1)\n",
        "labels = np.array(small_eval_dataset['labels'])\n",
        "wrong_predictions_idx = (predicted_labels != labels).nonzero()[0]\n",
        "wrong_idx = np.random.choice(wrong_predictions_idx, size=5, replace=False)\n",
        "wrong_texts = np.array(small_eval_dataset['text'])[wrong_idx]\n",
        "for i, (idx, text) in enumerate(zip(wrong_idx, wrong_texts)):\n",
        "  print(f\"text {i}:\")\n",
        "  print(f\"prediction: {predicted_labels[idx]}, label: {labels[idx]}\")\n",
        "  print(f\"model output: {predictions[0][idx]}\")\n",
        "  print(text)\n"
      ],
      "metadata": {
        "id": "kk2h2_VT7BlP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}