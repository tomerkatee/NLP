#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language hebrew
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "David"
\font_sans "default" "David"
\font_typewriter "default" "David"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing other 1.2
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1cm
\topmargin 2cm
\rightmargin 1cm
\bottommargin 3cm
\headheight 0cm
\headsep 0cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Box Doublebox
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "20pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\paragraph_spacing double
\align center

\series bold
\size huge
עיבוד שפה טבעית | תרגיל
\family roman
\shape up
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\shape default
\emph default
\numeric on
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
3
\end_layout

\begin_layout Plain Layout
\align center
מגישים:
\begin_inset Newline newline
\end_inset

תומר קטיעי
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\numeric on
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Newline newline
\end_inset


\numeric off
אייבי כץ
\numeric on

\begin_inset Newline newline
\end_inset


\numeric off
אחמד חלאילה
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
\lang english

\begin_inset Newline newline
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section*
חלק
\family roman
\series bold
\shape up
\size larger
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\numeric on
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
1
\end_layout

\begin_layout Standard
חלק זה הוא בעברית, סליחה אבל הסתבכנו עם התרגום שלו בליך, שאר החלקים יהיו
 באנגלית.
\end_layout

\begin_layout Subsection*
סעיף
\family roman
\series bold
\shape up
\size large
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
\lang english
a
\end_layout

\begin_layout Standard
ראשית נעביר את ה-
\lang english
Cross Entropy Loss
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\lang hebrew
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
לצורה שיותר נוח לגזור
\begin_inset Formula 
\[
CE(y,\hat{y})=-\sum_{i}y_{i}\cdot\log(\hat{y_{i}})=-\sum_{i}y_{i}\cdot\log(softmax(\theta)_{i})=-\sum_{i}y_{i}\cdot\log\left(\frac{\exp(\theta_{i})}{\sum_{j}\exp(\theta_{j})}\right)=-\sum_{i}y_{i}\cdot\left(\log\left(\exp(\theta_{i})\right)-\log\left(\sum_{j}\exp(\theta_{j})\right)\right)
\]

\end_inset


\begin_inset Formula 
\[
=-\sum_{i}y_{i}\cdot\log\left(\exp(\theta_{i})\right)+\log\left(\sum_{j}\exp(\theta_{j})\right)\sum_{i}y_{i}=-\sum_{i}y_{i}\theta_{i}+\log\left(\sum_{j}\exp(\theta_{j})\right)
\]

\end_inset

כעת נגזור לפי 
\begin_inset Formula $\theta_{k}$
\end_inset

 לכל 
\begin_inset Formula $k$
\end_inset

: 
\begin_inset Formula 
\[
\frac{\partial CE(y,\hat{y})}{\partial\theta_{k}}=-y_{k}+\frac{\exp(\theta_{k})}{\sum_{j}\exp(\theta_{j})}=-y_{k}+softmax(\theta)_{k}
\]

\end_inset

כלומר הגרדיינט הוא
\begin_inset Formula 
\[
\frac{\partial CE(y,\hat{y})}{\partial\theta}=-y+softmax(\theta)=\hat{y}-y
\]

\end_inset


\end_layout

\begin_layout Subsection*
סעיף
\family roman
\series bold
\shape up
\size large
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
\lang english
b
\end_layout

\begin_layout Standard
ניעזר בכלל השרשרת:
\begin_inset Formula 
\[
\frac{\partial J}{\partial x}=\frac{\partial CE(\hat{y}-y)}{\partial x}=\frac{\partial CE(\hat{y}-y)}{\partial(hW_{2}+b_{2})}\cdot\frac{\partial(hW_{2}+b_{2})}{\partial h}\cdot\frac{\partial h}{\partial(xW_{1}+b_{1})}\cdot\frac{\partial(xW_{1}+b_{1})}{\partial x}
\]

\end_inset


\begin_inset Formula 
\[
=(\hat{y}-y)\cdot W_{2}^{T}\cdot h(1-h)\cdot W_{1}^{T}
\]

\end_inset


\end_layout

\begin_layout Subsection*
סעיף
\family roman
\series bold
\shape up
\size large
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
\lang english
c
\end_layout

\begin_layout Standard
הקוד מומש ב-
\lang english
q1c_neural.py
\lang hebrew
.
\end_layout

\begin_layout Subsection*
סעיף
\family roman
\series bold
\shape up
\size large
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
\lang english
d
\end_layout

\begin_layout Standard
לאחר מימוש הקוד ב-
\lang english
q1d_neural_lm.py
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\lang hebrew
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
הרצנו
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
\lang english
SGD
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\lang hebrew
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
ל-
\numeric on
40000
\family roman
\series medium
\shape up
\size normal
\emph off
\numeric off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
איטרציות וקיבלנו
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
\lang english
dev-perplexity = 113.313
\lang hebrew
.
\end_layout

\begin_layout Section*

\lang english
section 2
\end_layout

\begin_layout Subsection*

\lang english
part a
\end_layout

\begin_layout Standard

\lang english
The advantage of a character-based model is that it has far fewer parameters
 to train compared to a word-based model like skipgram.
 This is because there are significantly fewer characters (e.g., if working
 with ASCII characters) than words.
 Another advantage is the ability to read and generate words that are not
 in the predefined vocabulary, such as the ability to read misspellings
 and work with new names.
 On the other hand, an advantage of a word-based model is that there is
 no risk of misspellings or invented words when generating text (since the
 valid words are predefined).
 Another advantage is the ability to base the model on the semantic proximity
 of words, similar to how humans think about language: humans do not rely
 on the relationship between letters when they speak and read, but on the
 relationship between the words themselves.
\end_layout

\begin_layout Subsection*

\lang english
part b
\end_layout

\begin_layout Standard

\lang english
We have completed the code in the attached .ipynb file.
\end_layout

\begin_layout Section*

\lang english
section 3
\end_layout

\begin_layout Subsection*

\lang english
part a
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
2^{-\frac{1}{M}\sum_{i=1}^{M}\log_{2}p(s_{i}\mid s_{1},\ldots,s_{i-1})}=2^{-\frac{1}{M}\log_{2}\left(\prod_{i=1}^{M}p(s_{i}\mid s_{1},\ldots,s_{i-1})\right)}=\left(2^{\log_{2}\left(\prod_{i=1}^{M}p(s_{i}\mid s_{1},\ldots,s_{i-1})\right)}\right)^{-\frac{1}{M}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\left(\prod_{i=1}^{M}p(s_{i}\mid s_{1},\ldots,s_{i-1})\right)^{-\frac{1}{M}}=\left(e^{\ln\left(\prod_{i=1}^{M}p(s_{i}\mid s_{1},\ldots,s_{i-1})\right)}\right)^{-\frac{1}{M}}=e^{-\frac{1}{M}\ln\left(\prod_{i=1}^{M}p(s_{i}\mid s_{1},\ldots,s_{i-1})\right)}=e^{-\frac{1}{M}\sum_{i=1}^{M}\ln p(s_{i}\mid s_{1},\ldots,s_{i-1})}
\]

\end_inset


\end_layout

\begin_layout Subsection*

\lang english
part b
\end_layout

\begin_layout Standard

\lang english
For the bi-gram LM model we got 111.72 perplexity for the Shakespeare data,
 and 82.90 for the Wikipedia data.
\begin_inset Newline newline
\end_inset

As for the model from the previous section, we got 7.15 perplexity for the
 Shakespeare data, and 18.35 for the Wikipedia data.
\end_layout

\begin_layout Subsection*

\lang english
part c
\end_layout

\begin_layout Standard

\lang english
In the results we noticed various differences.
\begin_inset Newline newline
\end_inset

Firstly the difference between Shakespeare data and Wikipedia data perplexity
 in the Bi-Gram model is due to the model being trained on more 'regular'
 text (in a brief look it seems like some newspaper text) which is closer
 to what Wikipedia contains, in contrast to Shakespeare texts carrying a
 unique style of writing that doesn't use every day words like Wikipedia.
\begin_inset Newline newline
\end_inset

We can also notice a symmetric phenomenon in the character-based model from
 the previous section: it is trained on Shakespeare texts and thus performs
 better on them in comparison to Wikipedia.
\begin_inset Newline newline
\end_inset

Another thing we can notice is the difference in the magnitude of the perplexiti
es.
 This is due to the second model being character-based, offering less options
 for the next token and resulting in a smaller perplexity, as opposed to
 the first being word-based.
 There are way more words than characters and so the perplexity is way higher.
\end_layout

\begin_layout Section*

\lang english
section 5
\end_layout

\begin_layout Subsection*

\lang english
part 1
\end_layout

\begin_layout Standard

\lang english
The authors of the article made the T5 (short for Text-to-Text Transfer
 Transformer) model publicly available.
 The dataset used to benchmark sentiment analysis is SST-2.
 In order to evaluate success in this task we measure accuracy (percentage
 of correct predictions) of the trained model over a test set.
\end_layout

\begin_layout Subsection*

\lang english
part 2
\end_layout

\begin_layout Standard

\lang english
We used this model: 
\begin_inset Quotes eld
\end_inset

michelecafagna26/t5-base-finetuned-sst2-sentiment
\begin_inset Quotes erd
\end_inset

 from hugging-face.
\begin_inset Newline newline
\end_inset

Then we tested the 4 given sentences and got these results:
\begin_inset Newline newline
\end_inset

1.
 
\begin_inset Quotes eld
\end_inset

This movie is awesome
\begin_inset Quotes erd
\end_inset

 - positive
\begin_inset Newline newline
\end_inset

2.
 
\begin_inset Quotes eld
\end_inset

I didn’t like the movie so much
\begin_inset Quotes erd
\end_inset

 - negative
\begin_inset Newline newline
\end_inset

3.
 
\begin_inset Quotes eld
\end_inset

I’m not sure what I think about this movie.
\begin_inset Quotes erd
\end_inset

 - negative
\begin_inset Newline newline
\end_inset

4.
 
\begin_inset Quotes eld
\end_inset

Did you like the movie?
\begin_inset Quotes erd
\end_inset

 - positive
\end_layout

\begin_layout Subsection*

\lang english
part 3
\end_layout

\begin_layout Standard

\lang english
loading the SST2 dataset and evaluating the accuracy of the T5 model yielded
 a 94.95% accuracy.
 This indeed matches the results shown on Papers with Code, which are around
 95%.
\end_layout

\begin_layout Subsection*

\lang english
part 4
\end_layout

\begin_layout Standard

\lang english
The data in the SST2 dataset is quite balanced.
 We measured (in code) that about 51% of the validation set is labeled positive.
 That is very important when evaluating the model's performace, because
 otherwise a model biased positively/negatively might mistakenly look better
 (in terms of accuracy on the unbalanced dataset) than a well-balanced model
 although the real-life distribution of positive-negative reviews is balanced.
 The bottom line is that the dataset distribution should represent the real-life
 distribution and apparently SST2 achieves that.
\end_layout

\begin_layout Subsection*

\lang english
part 5
\end_layout

\begin_layout Standard

\lang english
Simply testing the accuracy can generally work, but there are other metrics
 that a human can think of that help evaluating a model's quality.
 One non-trivial capability that one would like to test for is a model's
 ability to detect 
\series bold
sarcasm
\series default
 and label a review negative although it might contain many 'positive' words.
 Another example is to check for a model's response to references to other
 movies that are considered good/bad, for instance: 
\begin_inset Quotes eld
\end_inset

This movie reminds me of Titanic
\begin_inset Quotes erd
\end_inset

 should be classified as a positive review.
\end_layout

\end_body
\end_document
